{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE 572: Lab 8\n",
    "\n",
    "In this lab, you will practice implementing a logistic regression classifier from scratch and using Scikit-learn.\n",
    "\n",
    "To execute and make changes to this notebook, click File > Save a copy to save your own version in your Google Drive or Github. Read the step-by-step instructions below carefully. To execute the code, click on each cell below and press the SHIFT-ENTER keys simultaneously or by clicking the Play button. \n",
    "\n",
    "When you finish executing all code/exercises, save your notebook then download a copy (.ipynb file). Submit the following **three** things:\n",
    "1. a link to your Colab notebook,\n",
    "2. the .ipynb file, and\n",
    "3. a pdf of the executed notebook on Canvas.\n",
    "\n",
    "To generate a pdf of the notebook, click File > Print > Save as PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create toy dataset\n",
    "\n",
    "Below we create a toy dataset using the `make_classification()` function from Scikit-learn. You can read more about the arguments used to create the dataset in the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0, \n",
    "                           n_informative=2, random_state=1, \n",
    "                           n_clusters_per_class=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.scatter(X[:,0], X[:,1], c=y, alpha=0.6)\n",
    "ax.set_xlabel('$x_1$');\n",
    "ax.set_ylabel('$x_2$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize the feature values in the dataset\n",
    "\n",
    "Standardize the input features by subtracting the **feature-wise** mean and dividing by the **feature-wise** standard deviation.\n",
    "\n",
    "This means you must compute the mean and standard deviation along the feature axis of the data. The variable `X` has dimension `[n, m]` where `n` is the number of samples and `m` is the number of features. By default, `np.mean()` and `np.std()` compute the mean and std of the flattened array and thus return a single value. To return a value for each of the features, you must specify the `axis` argument to be along the feature axis (thus `np.mean()` and `np.std()` should return an array of two values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the inputs\n",
    "import numpy as np\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the dataset after standardizing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.scatter(X[:,0], X[:,1], c=y, alpha=0.6)\n",
    "ax.set_xlabel('$x_1$');\n",
    "ax.set_ylabel('$x_2$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the logistic function\n",
    "\n",
    "Write a function that returns the output of the logistic function. Write the code for the equation (do not use a library to import the function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the loss function\n",
    "\n",
    "Recall that the loss function for logistic regression is the log loss or cross-entropy function, which we will average over the samples:\n",
    "\n",
    "$L= \\frac{1}{n} \\sum_{i=1}^{n} L(\\hat{y}_i, y_i) = -\\frac{1}{n} \\sum_{i=1}^{n} [(y_i\\log(\\hat{y_i}) + (1-y_i)\\log(1-\\hat{y_i})] $\n",
    "\n",
    "The below function returns the cross entropy loss given a set of class labels $y$ and the predicted classes $\\hat{y}$. Write the equation for cross entropy loss in the function (do not use a library to import the function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y, y_hat):\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the gradients\n",
    "\n",
    "For gradient descent, we need to calculate the gradient of the loss as a function of the weights/parameters. The below function returns the gradient of the parameters w and b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradients(X, y, y_hat):\n",
    "    \n",
    "    # n is number of training examples\n",
    "    n = X.shape[0]\n",
    "    \n",
    "    # gradient of loss w.r.t weights\n",
    "    dw = (1/n)*np.dot(X.T, (y_hat - y))\n",
    "    \n",
    "    # gradient of loss w.r.t bias\n",
    "    db = (1/n)*np.sum((y_hat - y)) \n",
    "    \n",
    "    return dw, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model (learn parameters)\n",
    "\n",
    "The below function learns the parameters w and b using mini-batch stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sgd(X, y, batchsize, epochs, lr):\n",
    "    \n",
    "    # X: input data\n",
    "    # y: true class/target value\n",
    "    # batchsize: number of samples in each batch\n",
    "    # epochs: number of epochs (complete passes through training data)\n",
    "    # lr: learning rate\n",
    "        \n",
    "    # n: number of training examples\n",
    "    # m: number of features \n",
    "    n, m = X.shape\n",
    "    \n",
    "    # Initialize weights and bias to zeros\n",
    "    w = np.zeros((m,1))\n",
    "    b = 1\n",
    "    \n",
    "    # Reshape y to be an n x 1 vector for multiplication\n",
    "    y = y.reshape(n,1)\n",
    "    \n",
    "    # Empty list to store loss history\n",
    "    losses = []\n",
    "    \n",
    "    # Training loop\n",
    "    for i in range(epochs):\n",
    "        # Loop through each batch in the complete dataset\n",
    "        for j in range((n-1) // batchsize + 1):\n",
    "            \n",
    "            # Load a batch of data\n",
    "            start_i = j*batchsize\n",
    "            end_i = start_i + batchsize\n",
    "            xbatch = X[start_i:end_i]\n",
    "            ybatch = y[start_i:end_i]\n",
    "            \n",
    "            # Calculate prediction\n",
    "            y_hat = sigmoid(np.dot(xbatch, w) + b)\n",
    "            \n",
    "            # Get the gradients of loss w.r.t parameters\n",
    "            dw, db = gradients(xbatch, ybatch, y_hat)\n",
    "            \n",
    "            # Update the parameters\n",
    "            w = w - lr*dw\n",
    "            b = b - lr*db\n",
    "        \n",
    "        # Calculate loss and append it to the list for plotting later\n",
    "        l = loss(y, sigmoid(np.dot(X, w) + b))\n",
    "        losses.append(l)\n",
    "        \n",
    "    # return learned weights, bias, and list of losses\n",
    "    return w, b, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "w, b, loss_history = train_sgd(X, y, batchsize=100, epochs=1000, lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the decision boundary\n",
    "\n",
    "The following function plots the decision boundary learned for classifying our 2-dimensional dataset X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(X, w, b):\n",
    "    \n",
    "    # The line we need to plot is y=mx+c\n",
    "    # We equate mx + c = w.X + b\n",
    "    # Solve to find m and c\n",
    "    x1 = [min(X[:,0]), max(X[:,0])]\n",
    "    m = -w[0]/w[1]\n",
    "    c = -b/w[1]\n",
    "    x2 = m*x1 + c\n",
    "    \n",
    "    # Plotting\n",
    "    fig = plt.figure(figsize=(10,8))\n",
    "    plt.scatter(X[:,0], X[:,1], c=y, alpha=0.6)\n",
    "    plt.xlabel(\"$x_1$\")\n",
    "    plt.ylabel(\"$x_2$\")\n",
    "    plt.title('Decision boundary')\n",
    "    plt.plot(x1, x2, 'r-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(X, w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions for training set\n",
    "\n",
    "The below function makes predictions for a set of data instances and thresholds the model output (which ranges from [0,1]) to a binary output (which has values of 0 or 1). If the model output is >= 0.5, we predict y=1, else we predict y=0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w, b):\n",
    "    # Calculate predictions using model parameters w, b\n",
    "    preds = sigmoid(np.dot(X, w) + b)\n",
    "    \n",
    "    # if y_hat >= 0.5 --> round up to 1\n",
    "    # if y_hat < 0.5 --> round up to 1\n",
    "    pred_class = [1 if i >= 0.5 else 0 for i in preds]\n",
    "    \n",
    "    return np.array(pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_train = predict(X, w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the loss history\n",
    "\n",
    "The below function takes a list of losses from the training history and plots them as a function of the number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_history(losses):\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.plot(range(1, len(losses)+1), losses)\n",
    "    ax.set_xlabel('Number of epochs')\n",
    "    ax.set_ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_history(loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1: Try changing the learning rate to different values, e.g. 0.1, 0.01, and 0.001. What happens to the plot of the loss history as the learning rate increases?**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute training accuracy\n",
    "\n",
    "Compute and print the accuracy on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression using sklearn\n",
    "\n",
    "In this section, we'll implement logistic regression for the same classification task using scikit-learn instead of writing the code from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a logistic regression classifier and fit it to the training data\n",
    "clf = LogisticRegression(random_state=0)\n",
    "clf = clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute and print the classification accuracy for the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Now you've learned how to implement logistic regression from scratch in python as well as using scikit-learn. Using both methods we got the same classification accuracy on the training set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
