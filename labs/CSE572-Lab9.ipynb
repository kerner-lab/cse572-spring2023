{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE 572: Lab 9\n",
    "\n",
    "In this lab, you will practice implementing feed-forward and convolutional neural networks.\n",
    "\n",
    "To execute and make changes to this notebook, click File > Save a copy to save your own version in your Google Drive or Github. Read the step-by-step instructions below carefully. To execute the code, click on each cell below and press the SHIFT-ENTER keys simultaneously or by clicking the Play button. \n",
    "\n",
    "When you finish executing all code/exercises, save your notebook then download a copy (.ipynb file). Submit the following **three** things:\n",
    "1. a link to your Colab notebook,\n",
    "2. the .ipynb file, and\n",
    "3. a pdf of the executed notebook on Canvas.\n",
    "\n",
    "To generate a pdf of the notebook, click File > Print > Save as PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "seed = 0\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-forward neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset\n",
    "\n",
    "For this example, we will use the [Cleveland Heart Disease dataset](https://archive.ics.uci.edu/ml/datasets/heart+Disease). Review the dataset documentation to learn more about the attributes and other aspects of the dataset. The dataset consists of a CSV file with 303 rows. Each row contains information about a patient. There are 14 attribute columns and one binary class column (`target`) that reports whether or not a patient had a heart disease. We will train a feed-forward neural network model to predict whether or not a given patient has a heart disease based on the attribute values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(\"http://storage.googleapis.com/download.tensorflow.org/data/heart.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print sample rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of rows and columns\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into three randomly-sampled subsets: training (60%), validation (20%), and test (20%). Use the `seed` variable for the `random_state`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = np.split(data.sample(frac=1, random_state=seed), [int(.6*len(data)), int(.8*len(data))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the number of samples in each of the three subsets and the number of instances from each class. For example, for the training set you might print \"The training set has __ instances (__ heart disease, __ no disease)\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the dataset\n",
    "\n",
    "Before we can feed this dataset to our model for training and evaluation, we need to perform a few steps to get it ready:\n",
    "1. Convert the dataframes to Dataset objects\n",
    "2. Normalize the numerical feature values\n",
    "3. Binarize the Categorical features by converting to one-hot encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataframes to Dataset objects\n",
    "def dataframe_to_dataset(df):\n",
    "    df = df.copy()\n",
    "    # Remove the target column and store in a separate array\n",
    "    labels = df.pop('target')\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))\n",
    "    ds = ds.shuffle(buffer_size=len(df), reshuffle_each_iteration=False)\n",
    "    return ds\n",
    "\n",
    "\n",
    "train_ds = dataframe_to_dataset(train)\n",
    "val_ds = dataframe_to_dataset(val)\n",
    "test_ds = dataframe_to_dataset(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dataset object yields a tuple containing the input feature vector and target (class value): `(input, target)`. `input` is a dictionary of features and `target` is the value 0 or 1. The code below prints an example instance drawn from the training Dataset object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in train_ds.take(1):\n",
    "    print(\"Input:\", x)\n",
    "    print(\"\\nTarget:\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the batch() function in keras to create batches from the full dataset for passing to the model. For the training dataset, we'll define a hyperparameter `batch_size` that we will set. For the validation and test sets, we will make the batch size equivalent to the size of the subset so all samples in that subset are evaluated each time the dataset is evaluated by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_ds = train_ds.batch(batch_size)\n",
    "val_ds = val_ds.batch(val.shape[0])\n",
    "test_ds = test_ds.batch(test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are seven categorical features in the dataset: `sex`, `cp`, `fbs`, `restecg`, `exang`, `ca`, and `thal`. You can read more about what these features mean in the [dataset documentation](https://archive.ics.uci.edu/ml/datasets/heart+Disease). All of them except `thal` have integer data type while `thal` has String data type. Below we define a function to encode these feature values as one-hot encodings using the [IntegerLookup()](https://www.tensorflow.org/api_docs/python/tf/keras/layers/IntegerLookup) and [StringLookup()](https://www.tensorflow.org/api_docs/python/tf/keras/layers/StringLookup) layers. These layers create look-up tables for mapping a set of arbitrary integers or strings to a one-hot encoding. We use an `is_string` argument to indicate whether we should use the `StringLookup()` for `thal` or the `IntegerLookup()` for the remaining features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categorical_feature(feature, name, dataset, is_string):\n",
    "    from tensorflow.keras.layers import IntegerLookup\n",
    "    from tensorflow.keras.layers import StringLookup\n",
    "    \n",
    "    # Create lookup layer to turn categorical features into 1-hot integer encodings\n",
    "    if is_string:\n",
    "        lookup = StringLookup(output_mode=\"binary\")\n",
    "    else:\n",
    "        lookup = IntegerLookup(output_mode=\"binary\")\n",
    "\n",
    "    # Prepare a Dataset that only yields the feature of interest\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
    "\n",
    "    # Find the set of possible values and assign them a fixed integer index\n",
    "    lookup.adapt(feature_ds)\n",
    "\n",
    "    # Turn the input into integer indices\n",
    "    encoded_feature = lookup(feature)\n",
    "    return encoded_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining features in the dataset (`age`, `trestbps`, `chol`, `thalach`, `oldpeak`, and `slope`) are all numerical measurements. You can read more about what these features mean in the [dataset documentation](https://archive.ics.uci.edu/ml/datasets/heart+Disease). We don't need to encode the numerical features, but we do want to scale them to the same range of values (e.g., using standardization or normalization). Below we define a function that uses the [Normalization()](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Normalization) layer to standardize the data (subtract the mean and divide by the standard deviation for each feature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(feature, name, dataset):\n",
    "    from tensorflow.keras.layers import Normalization\n",
    "    \n",
    "    # Create a Normalization layer for our feature\n",
    "    normalizer = Normalization()\n",
    "\n",
    "    # Prepare a Dataset that only yields the feature of interest\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
    "\n",
    "    # Learn the statistics of the data\n",
    "    normalizer.adapt(feature_ds)\n",
    "\n",
    "    # Normalize the input feature\n",
    "    norm_feature = normalizer(feature)\n",
    "    return norm_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply these functions to each of our features and return an encoded/preprocessed input layer. We first create tensor variables for each of the inputs, then apply the appropriate function, then concatenate all of these input layers for each feature together to form a single input feature layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_inputs = [\n",
    "    keras.Input(shape=(1,), name=\"sex\", dtype=\"int64\"),\n",
    "    keras.Input(shape=(1,), name=\"cp\", dtype=\"int64\"),\n",
    "    keras.Input(shape=(1,), name=\"fbs\", dtype=\"int64\"),\n",
    "    keras.Input(shape=(1,), name=\"restecg\", dtype=\"int64\"),\n",
    "    keras.Input(shape=(1,), name=\"exang\", dtype=\"int64\"),\n",
    "    keras.Input(shape=(1,), name=\"ca\", dtype=\"int64\"),\n",
    "    keras.Input(shape=(1,), name=\"thal\", dtype=\"string\"),\n",
    "    keras.Input(shape=(1,), name=\"age\"),\n",
    "    keras.Input(shape=(1,), name=\"trestbps\"),\n",
    "    keras.Input(shape=(1,), name=\"chol\"),\n",
    "    keras.Input(shape=(1,), name=\"thalach\"),\n",
    "    keras.Input(shape=(1,), name=\"oldpeak\"),\n",
    "    keras.Input(shape=(1,), name=\"slope\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_layer = layers.concatenate(\n",
    "    [\n",
    "        encode_categorical_feature(all_inputs[0], \"sex\", train_ds, False),\n",
    "        encode_categorical_feature(all_inputs[1], \"cp\", train_ds, False),\n",
    "        encode_categorical_feature(all_inputs[2], \"fbs\", train_ds, False),\n",
    "        encode_categorical_feature(all_inputs[3], \"restecg\", train_ds, False),\n",
    "        encode_categorical_feature(all_inputs[4], \"exang\", train_ds, False),\n",
    "        encode_categorical_feature(all_inputs[5], \"ca\", train_ds, False),\n",
    "        encode_categorical_feature(all_inputs[6], \"thal\", train_ds, True),\n",
    "        normalize(all_inputs[7], \"age\", train_ds),\n",
    "        normalize(all_inputs[8], \"trestbps\", train_ds),\n",
    "        normalize(all_inputs[9], \"chol\", train_ds),\n",
    "        normalize(all_inputs[10], \"thalach\", train_ds),\n",
    "        normalize(all_inputs[11], \"oldpeak\", train_ds),\n",
    "        normalize(all_inputs[12], \"slope\", train_ds)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model\n",
    "\n",
    "Now that we've prepared our dataset, we can construct our neural network model. We construct the model by composing Layer objects starting with the input layer (which we've already defined as `feature_layer`) and ending with the output layer (which will be the final output of the model). In this example, we will only use [Dense()](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) layers which are simple fully-connected feed-forward layers (i.e., each output from layer `i-1` is connected by a weight variable to every neuron in layer `i`). We will create a network with only one hidden layer (between the input and output layers).\n",
    "\n",
    "The Dense() layer object allows us to specify the activation function to use using the `activation` argument. In class, we talked about several activation functions including sigmoid, sign, and tanh. Another commonly used activation function is the rectified linear unit, or \"ReLU\" function. Another commonly used activation function is the rectified linear unit, or \"ReLU\" function, which has the equation $a(z)=max(0,z)$. We will use `relu` as our activation function in this example for all layers except the final layer, which will use a `sigmoid` activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a variable for the number of units/neurons in the layer\n",
    "h1_units = 32\n",
    "# Create a Dense layer and append it to the input layer\n",
    "h1_layer = layers.Dense(h1_units, activation='relu')(feature_layer)\n",
    "\n",
    "# Create an output layer with one output representing the likelihood of \n",
    "# heart disease and append it to the hidden layer\n",
    "output_layer = layers.Dense(1, activation=\"sigmoid\")(h1_layer)\n",
    "\n",
    "# Build the model specifying the input and output layer\n",
    "model = keras.Model(inputs=all_inputs, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot our completed model to visualize the input, hidden, and output layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `rankdir='LR'` is to make the graph horizontal\n",
    "keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we compile the model by specifying the optimization technique and loss function to be used in model training. We can also specify the metric(s) that will be logged during training. We will use stochastic gradient descent (`sgd`) for the optimizer and binary cross entropy (log loss) as the loss function. We will log the accuracy metric during training. We can also specify the learning rate hyperparameter here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "model.compile(keras.optimizers.SGD(learning_rate=lr), \"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "Now that we've constructed and compiled our model, we can train the model using our training dataset. This is done in keras using the `fit()` function, which also gives us an option to provide the validation dataset which will be used to evaluate validation accuracy after every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_result = model.fit(train_ds, epochs=100, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fit()` function returns a history attribute that gives the metrics recorded during training as a dictionary. We can print the dictionary keys to see which metrics were stored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_result.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a figure with two subplots. The first subplot should plot the training and validation loss (`loss` and `val_loss`) and the second subplot should plot the training and validation accuracy (`accuracy` and `val_accuracy`). Make sure you include the axis labels and a legend in each plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10,5))\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model\n",
    "\n",
    "Finally, we evaluate our trained model on the held-out test set. First we predict the outputs for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model output from the final sigmoid layer is a value between 0 and 1 representing the likelihood that a given sample patient has heart disease. To get the predicted classes, we predict 1 if the output was >= 0.5 and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_classes = [1 if p >= 0.5 else 0 for p in preds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute and print the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1: There is a large difference between the training and validation accuracies and the test accuracy. What do you think could explain this difference?**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "YOUR ANSWER HERE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional neural networks\n",
    "\n",
    "The previous example used a structured dataset of pre-determined features to predict the likelihood of heart disease in a patient. This type of dataset is well suited for a feed-forward neural network (FFNN) architecture. However, other datasets may not be as well suited for this type of architecture. For example, to train a FFNN to classify an image dataset, we would need to convert each image (which has dimension $N \\times M \\times K$ where $N$ and $M$ are the image dimensions and $K$ is the number of color channels - 3 for RGB) to a feature vector representation. We can do this by flattening the image to a 1D feature vector so the new dimension would be $N * M * K \\times 1$, but this would remove the 2D spatial structure.\n",
    "\n",
    "Convolutional neural networks (CNNs) are another type of neural network architecture that enables spatial relationships to be learned from the data. The features learned in each hidden layer have 2D structures that act as filters on the spatial inputs, rather than scalar activations like in a FFNN. In this example, we'll train a CNN to classify an image dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset\n",
    "\n",
    "For this example we'll use the [FashionMNIST dataset](https://keras.io/api/datasets/fashion_mnist/). This is a dataset of 60,000 28x28 color training images and 10,000 test images with 10 class labels:\n",
    "\n",
    "0.\tt-shirt\n",
    "1.\ttrouser\n",
    "2.\tpullover\n",
    "3.\tdress\n",
    "4.\tcoat\n",
    "5.\tsandal\n",
    "6.\tshirt\n",
    "7.\tsneaker\n",
    "8.\tbag\n",
    "9.\tboot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the dimensions of the datasets\n",
    "\n",
    "print('Training data input dim: ', x_train.shape)\n",
    "print('Training data label dim: ', y_train.shape)\n",
    "print('Test data input dim: ', x_test.shape)\n",
    "print('Test data label dim: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set variable for number of classes\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll plot a few random images from the training set to get an idea of what the data look like. You can run this as many times as you want to see a different random subset of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of images to plot\n",
    "n = 5\n",
    "\n",
    "# create a dictionary for the label names\n",
    "labels = {\n",
    "    0: 't-shirt',\n",
    "    1: 'trouser',\n",
    "    2: 'pullover',\n",
    "    3: 'dress',\n",
    "    4: 'coat',\n",
    "    5: 'sandal',\n",
    "    6: 'shirt',\n",
    "    7: 'sneaker',\n",
    "    8: 'bag',\n",
    "    9: 'boot'\n",
    "}\n",
    "\n",
    "\n",
    "# Randomly sample n sample indices\n",
    "rand_inds = np.random.randint(0, x_train.shape[0], n)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=n)\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(x_train[rand_inds[i]], cmap='gray')\n",
    "    ax.set_title(labels[y_train[rand_inds[i]]])\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class is currently a categorical variable with values 0 to 9. We need to convert this to a one-hot integer encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class category to one hot vector\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model\n",
    "\n",
    "We will construct a model in the same way as the previous example by composing layers starting with the input layer and ending with the output layer. However in this example, we will use [Conv2D()](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) layers instead of Dense() layers as our hidden layers. In addition to specifying the number of neurons/filters and activation function for each Conv2D() layer, we need to specify the kernel size (the size of the convolutional kernel/filter). We will use $3\\times3$ convolutions for all layers in this example. Each Conv2D() layer is followed by a [MaxPool2D()](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D) layer to enable invariance to small translations.\n",
    "\n",
    "We will still use a Dense() layer for the final output layer. Previously we used the sigmoid activation in our final layer, but this is only used for binary classification outputs (0 to 1). Since this is a multi-class classification, we will use the [softmax function](https://www.tensorflow.org/api_docs/python/tf/keras/activations/softmax) which returns the log odds for the multi-class case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input layer\n",
    "input_layer = keras.Input(shape=[x_train.shape[1], x_train.shape[2], 1])\n",
    "\n",
    "# Define one hidden layer of Conv2D + Max Pooling\n",
    "h1 = layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\")(input_layer)\n",
    "h1 = layers.MaxPooling2D(pool_size=(2, 2))(h1)\n",
    "\n",
    "# Define a second Conv2D layer with 64 units, 3x3 kernel, and relu activation\n",
    "h2 = # YOUR CODE HERE\n",
    "\n",
    "# Define a second MaxPooling2D layer with 2x2 pool size\n",
    "h2 = # YOUR CODE HERE\n",
    "\n",
    "# Flatten the output from the last hidden layer so it can be passed to Dense layer\n",
    "h2 = layers.Flatten()(h2)\n",
    "\n",
    "# Final output layer\n",
    "output = layers.Dense(num_classes, activation=\"softmax\")(h2)\n",
    "\n",
    "# Build the model specifying the input and output layer\n",
    "model = keras.Model(inputs=input_layer, outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot our completed model to visualize the input, hidden, and output layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `rankdir='LR'` is to make the graph horizontal\n",
    "keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to visualize our model is to print a table of the layers and the shape of the representation at each layer using the `model.summary()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "Now that we've constructed our model, we can compile and train the model as in the previous example. Since we are doing a multi-class classification, we use the [categorical cross-entropy](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/categorical_crossentropy) loss function instead of binary-cross entropy. \n",
    "\n",
    "In the first example, we created our own train/val/test subsets. In this example, the FashionMNIST dataset provided a train/test split but not a validation subset. We can have keras automatically split out a fraction of the training data using the `validation_split` argument to `model.fit()` as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "epochs = 10\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
    "\n",
    "model_result = model.fit(x_train, y_train, \n",
    "                         batch_size=batch_size,\n",
    "                         epochs=epochs, \n",
    "                         validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a figure with two subplots. The first subplot should plot the training and validation loss (`loss` and `val_loss`) and the second subplot should plot the training and validation accuracy (`accuracy` and `val_accuracy`). Make sure you include the axis labels and a legend in each plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10,5))\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute and print the test set accuracy. Note that your final accuracy may be different each time you run this due to the randomness in the validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
