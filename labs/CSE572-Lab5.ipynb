{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE 572: Lab 5\n",
    "\n",
    "In this lab, you will practice implementing decision trees and random forests.\n",
    "\n",
    "To execute and make changes to this notebook, click File > Save a copy to save your own version in your Google Drive or Github. Read the step-by-step instructions below carefully. To execute the code, click on each cell below and press the SHIFT-ENTER keys simultaneously or by clicking the Play button. \n",
    "\n",
    "When you finish executing all code/exercises, save your notebook then download a copy (.ipynb file). Submit the following **three** things:\n",
    "1. a link to your Colab notebook,\n",
    "2. the .ipynb file, and\n",
    "3. a pdf of the executed notebook on Canvas.\n",
    "\n",
    "To generate a pdf of the notebook, click File > Print > Save as PDF.\n",
    "\n",
    "Acknowledgment: Much of the content in this notebook was adapted from Introduction to Data Mining, 2nd Edition by Tan, Steinbach, Karpatne, Kumar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vertebrate Dataset\n",
    "\n",
    "The vertebrate dataset we will use in this lab consists of samples containing information about vertebrates. Each vertebrate is classified into one of 5 categories: mammals, reptiles, birds, fishes, and amphibians, based on a set of explanatory attributes (predictor variables). Except for \"name\", the rest of the attributes have been converted into a  binary representation. To illustrate this, we will first load the data into a Pandas DataFrame object and display its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('https://docs.google.com/uc?export=download&id=1DrqbYx-0E8qdHexxO7m9fo11444pz5v5', header='infer')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the limited number of training examples (15), suppose we convert the problem into a binary classification task (mammals versus non-mammals). We can do so by replacing the class labels of the instances to *non-mammals* except for those that belong to the *mammals* class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Class'] = data['Class'].replace(['fishes','birds','amphibians','reptiles'], 'non-mammals')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the Pandas cross-tabulation function to examine the relationship between the Warm-blooded and Gives Birth attributes with respect to the class. This cross-tabulation gives the counts of mammals and non-mammals associated with each combination of Warm-blooded and Gives Birth values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab([data['Warm-blooded'], data['Gives Birth']], data['Class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above show that it is possible to distinguish mammals from non-mammals using these two attributes alone since each combination of their attribute values would yield only instances that belong to the same class. For example, mammals can be identified as warm-blooded vertebrates that give birth to their young. Such a relationship can also be derived using a decision tree classifier, as shown by the example given in the next subsection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier\n",
    "\n",
    "In this section, we apply a decision tree classifier to the vertebrate dataset described in the previous subsection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "Y = data['Class']\n",
    "X = data.drop(['Name','Class'], axis=1)\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=3)\n",
    "clf = clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceding commands will extract the predictor (X) and target class (Y) attributes from the vertebrate dataset and create a decision tree classifier object using entropy as its impurity measure for splitting criterion. The decision tree class in Python sklearn library also supports using 'gini' as impurity measure. The classifier above is also constrained to generate trees with a maximum depth equals to 3. Next, the classifier is trained on the labeled data using the fit() function. \n",
    "\n",
    "We can plot the resulting decision tree obtained after training the classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydotplus \n",
    "from IPython.display import Image\n",
    "\n",
    "dot_data = tree.export_graphviz(clf, feature_names=X.columns, class_names=['mammals','non-mammals'], filled=True, \n",
    "                                out_file=None) \n",
    "graph = pydotplus.graph_from_dot_data(dot_data) \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, suppose we apply the decision tree to classify the following test examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData = [['gila monster',0,0,0,0,1,1,'non-mammals'],\n",
    "           ['platypus',1,0,0,0,1,1,'mammals'],\n",
    "           ['owl',1,0,0,1,1,0,'non-mammals'],\n",
    "           ['dolphin',1,1,1,0,0,0,'mammals']]\n",
    "\n",
    "testData = pd.DataFrame(testData, columns=data.columns)\n",
    "testData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first extract the predictor and target class attributes from the test data and then apply the decision tree classifier to predict their classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testY = testData['Class']\n",
    "testX = testData.drop(['Name','Class'],axis=1)\n",
    "\n",
    "predY = clf.predict(testX)\n",
    "predictions = pd.concat([testData['Name'], pd.Series(predY,name='Predicted Class')], axis=1)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Except for platypus, which is an egg-laying mammal, the classifier correctly predicts the class label of the test examples. We can calculate the accuracy of the classifier on the test data as shown by the example given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Accuracy on test data is %.2f' % (accuracy_score(testY, predY)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Overfitting\n",
    "\n",
    "To illustrate the problem of model overfitting, we consider a synthetic two-dimensional dataset containing 1500 labeled instances, each of which is assigned to one of two classes, 0 or 1. Instances from each class are generated as follows:\n",
    "1. Instances from class 1 are generated from a mixture of 3 two-dimensional Gaussian distributions, centered at [6, 14], [10, 6], and [14, 14], respectively. \n",
    "2. Instances from class 0 are generated from a uniform distribution in a two-dimensional square region, whose sides have a length equal to 20.\n",
    "\n",
    "For simplicity, both classes have equal number of labeled instances. The code for generating and plotting the data is shown below. All instances from class 1 are shown in red while those from class 0 are shown in black."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import random\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "N = 1500\n",
    "\n",
    "mean1 = [6, 14]\n",
    "mean2 = [10, 6]\n",
    "mean3 = [14, 14]\n",
    "cov = [[3.5, 0], [0, 3.5]]  # diagonal covariance\n",
    "\n",
    "np.random.seed(50)\n",
    "# Generate the dataset from class 1\n",
    "X = np.random.multivariate_normal(mean1, cov, int(N/6))\n",
    "X = np.concatenate((X, np.random.multivariate_normal(mean2, cov, int(N/6))))\n",
    "X = np.concatenate((X, np.random.multivariate_normal(mean3, cov, int(N/6))))\n",
    "\n",
    "# Generate the dataset from class 0 and concatenate to the first dataset\n",
    "X = np.concatenate((X, 20*np.random.rand(int(N/2), 2)))\n",
    "\n",
    "# Assign the labels to classes 0 and 1\n",
    "Y = np.concatenate((np.ones(int(N/2)), np.zeros(int(N/2))))\n",
    "\n",
    "plt.plot(X[:int(N/2),0],X[:int(N/2),1],'r+',X[int(N/2):,0],X[int(N/2):,1],'k.',ms=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we reserve 80% of the labeled data for training and the remaining 20% for testing. We then fit decision trees of different maximum depths (from 2 to 50) to the training set and plot their respective accuracies when applied to the training and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# Training and Test set creation\n",
    "#########################################\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=1)\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#########################################\n",
    "# Model fitting and evaluation\n",
    "#########################################\n",
    "\n",
    "maxdepths = [2,3,4,5,6,7,8,9,10,15,20,25,30,35,40,45,50]\n",
    "\n",
    "trainAcc = np.zeros(len(maxdepths))\n",
    "testAcc = np.zeros(len(maxdepths))\n",
    "\n",
    "index = 0\n",
    "for depth in maxdepths:\n",
    "    clf = tree.DecisionTreeClassifier(max_depth=depth)\n",
    "    clf = clf.fit(X_train, Y_train)\n",
    "    Y_predTrain = clf.predict(X_train)\n",
    "    Y_predTest = clf.predict(X_test)\n",
    "    trainAcc[index] = accuracy_score(Y_train, Y_predTrain)\n",
    "    testAcc[index] = accuracy_score(Y_test, Y_predTest)\n",
    "    index += 1\n",
    "    \n",
    "#########################################\n",
    "# Plot of training and test accuracies\n",
    "#########################################\n",
    "    \n",
    "plt.plot(maxdepths,trainAcc,'ro-',maxdepths,testAcc,'bv--')\n",
    "plt.legend(['Training Accuracy','Test Accuracy'])\n",
    "plt.xlabel('Max depth')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1: What happens to the training accuracy as the model becomes more complex (maximum depth of tree increases)? What happens to the test accuracy?**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2: The model begins to overfit when the test accuracy starts to decrease while the training accuracy is still increasing. What is the approximate maximum depth at which the model starts to overfit?**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "A random forest is an ensemble of decision trees designed to improve generalization to unseen test data.\n",
    "\n",
    "In the example below, we fit a random forest with varying numbers of decision trees to the 2-dimensional dataset using each ensemble method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "\n",
    "maxdepth = 3\n",
    "n_trees = range(1, 101, 5)\n",
    "\n",
    "trainAcc = []\n",
    "testAcc = []\n",
    "\n",
    "for n in n_trees:\n",
    "    clf = ensemble.RandomForestClassifier(n_estimators=n)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    Y_predTrain = clf.predict(X_train)\n",
    "    Y_predTest = clf.predict(X_test)\n",
    "    trainAcc.append(accuracy_score(Y_train, Y_predTrain))\n",
    "    testAcc.append(accuracy_score(Y_test, Y_predTest))\n",
    "\n",
    "plt.plot(n_trees, trainAcc, 'ro-', n_trees, testAcc,'bv--')\n",
    "plt.legend(['Training Accuracy','Test Accuracy'])\n",
    "plt.xlabel('Number of trees')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the iris dataset used in Lab 2. Then perform the following steps:\n",
    "\n",
    "1. Split the dataset into 80% train and 20% test (use random state = 50). \n",
    "2. Train a decision tree with **max depth = 3 and using Gini index criterion** and print the resulting training and test accuracy. \n",
    "3. Plot the resulting decision tree obtained after training the classifier using graphviz as in the vertebrate example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header=None)\n",
    "data.columns = ['sepal length', 'sepal width', 'petal length', 'petal width', 'class']\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into 80% train and 20% test. Use a random state of 50 for reproducibility.\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train decision tree and print validation and test accuracy\n",
    "np.random.seed(50)\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot resulting decision tree\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3: In Lab 2, we came up with rules that could be used to separate instances from the different iris classes, such as:**\n",
    "- setosa if petal length < 2.5, virginica if petal length > 4.8, and versicolor otherwise \n",
    "- setosa if petal width < 1, virginica if petal width > 1.4, and versicolor otherwise \n",
    "\n",
    "**How do these rules compare to the splits learned by the decision tree?**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
