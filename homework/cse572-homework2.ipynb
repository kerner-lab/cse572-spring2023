{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86abac76",
   "metadata": {},
   "source": [
    "# CSE 572: Homework 2\n",
    "\n",
    "This notebook provides a template and starting code to implement the Homework 2 assignment.\n",
    "\n",
    "To execute and make changes to this notebook, click File > Save a copy to save your own version in your Google Drive or Github. Read the step-by-step instructions below carefully. To execute the code, click on each cell below and press the SHIFT-ENTER keys simultaneously or by clicking the Play button. \n",
    "\n",
    "When you finish executing all code/exercises, save your notebook then download a copy (.ipynb file). Submit the following **three** things:\n",
    "1. a link to your Colab notebook,\n",
    "2. the .ipynb file, and\n",
    "3. a pdf of the executed notebook on Canvas.\n",
    "\n",
    "To generate a pdf of the notebook, click File > Print > Save as PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c95671",
   "metadata": {},
   "source": [
    "## Prepare the dataset\n",
    "\n",
    "In this homework, you will compare the effect of multiple dimensionality reduction techniques on the classification performance for the [Covertype](https://archive.ics.uci.edu/ml/datasets/Covertype) dataset.  \n",
    "\n",
    "Each instance in this dataset represents a 30 m x 30 m patch of forested land described by 54 attributes. The attributes include features such as elevation, aspect, slope, soil characteristics, etc. Each sample has an associated forest cover type (e.g., douglas fir or Ponderosa pine) represented by an integer value 1 to 7. The dataset was created by the Department of Forest Sciences at Colorado State University and the US Forest Service in 1998. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f315a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "seed = 0\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a5e8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load and visualize a sample of the dataset\n",
    "data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz', header=None)\n",
    "\n",
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7e3caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the label column from the data matrix\n",
    "labels = data[data.columns[-1]]\n",
    "data = data[data.columns[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b920c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbdcf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3c9589",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056a96b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680c6d90",
   "metadata": {},
   "source": [
    "## Dimensionality reduction\n",
    "\n",
    "You will implement 2 dimensionality reduction techniques:\n",
    "- PCA (linear)\n",
    "- Autoencoder neural network (non-linear)\n",
    "\n",
    "For PCA, you will create a plot of the total fraction of explained variance by the first 1 through 30 principal components (as we did in Lab 12). Choose the number of principal components to retain based on the inflection point of this plot, i.e., the point at which the increase in total explained variance begins to plateau (as we did in Lab 12). Note that this may not be a sharp change in the curve; choose the number of components that you think is best based on the overall trend.\n",
    "\n",
    "For the autoencoder neural network, implement a network with the following layers:\n",
    "1. Input layer (# units = 54) [encoder]\n",
    "2. Hidden layer (# units = 32) [encoder]\n",
    "3. Hidden layer (# units = number of PCs retained for PCA) [encoded/bottleneck layer]\n",
    "4. Hidden layer (# units = 32) [decoder]\n",
    "5. Output layer (# units = 54) [decoder]\n",
    "\n",
    "For example, if you chose to use 3 principal components in PCA, you will have a bottleneck layer of 3 units in your autoencoder. \n",
    "\n",
    "Use ‘relu’ activation for hidden layers and ‘sigmoid’ activation for the output layer, ‘sgd’ (stochastic gradient descent) for the optimizer, and ‘mse’ (mean squared error) as the loss function. Train your model for 40 epochs with a batch size of 64. Lab 13 will be a useful guide for this implementation. Note that you will use the predict() function with only the encoder part of the model to transform your features into the encoded (reduced-dimension) representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b552def",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b608b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44530eff",
   "metadata": {},
   "source": [
    "### Autoencoder neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94356c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56413dc6",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "You will use a Random Forest classifier with 100 trees for the classification model (using Scikit-learn). Leave all other hyperparameters as their default values. You will train 3 separate random forest classifiers with 1) input data transformed using PCA, 2) input data transformed using autoencoder, 3) no dimensionality reduction (original data attributes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6725d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f713eb",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Your final model evaluation should be performed on the test set. You will compare the results of the two dimensionality reduction + kNN methods (PCA + kNN, Autoencoder + kNN) as well as a baseline kNN classifier that does selects a random subset of features from the original attributes. For each of the 3 methods, print the classification report (including class-wise precision, recall, F1 + overall accuracy) and plot the confusion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2986a2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4409dbf",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "Briefly summarize the results from your three compared models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
